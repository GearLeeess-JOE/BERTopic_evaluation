{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/joe/projects/tm-projects/topic-modeling/evaluation\") # 将module1所在的文件夹路径放入sys.path中\n",
    "sys.path.append(\"/home/joe/projects/tm-projects/topic-modeling/plugins/TSCTM/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eva_data import DataLoader\n",
    "from eva_evaluation import Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **LDA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = [\"20NewsGroup\"]\n",
    "\n",
    "for name in dataset_name:\n",
    "    if name == \"20NewsGroup\":\n",
    "        dataset, custom = name, False\n",
    "    else:\n",
    "        dataset, custom = name, True\n",
    "\n",
    "    params =  {\"num_topics\": [20, 30, 50]}\n",
    "    trainer = Trainer(dataset=dataset,\n",
    "                    model_name=\"LDA\",\n",
    "                    params=params,\n",
    "                    custom_dataset=custom,\n",
    "                    verbose=True)\n",
    "    results = trainer.train(save=f\"./extension_results/malletLDA_{name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l8s9zjC5A6JD"
   },
   "source": [
    "## **malletLDA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = [\"20NewsGroup\"]\n",
    "for i, random_state in enumerate([0]):\n",
    "    for name in dataset_name:\n",
    "        if name == \"20NewsGroup\":\n",
    "            dataset, custom = name, False\n",
    "        else:\n",
    "            dataset, custom = name, True\n",
    "\n",
    "        params =  {\"num_topics\": [20, 30, 50], \"random_seed\": random_state}\n",
    "        trainer = Trainer(dataset=dataset,\n",
    "                        model_name=\"malletLDA\",\n",
    "                        params=params,\n",
    "                        custom_dataset=custom,\n",
    "                        verbose=True,\n",
    "                        malletLDA_path=\"\")  # update this path\n",
    "        results = trainer.train(save=f\"./extension_results/malletLDA_{name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CGhfpkBZGtRl"
   },
   "source": [
    "## **BERTopic (MPNET)**\n",
    "\n",
    "To speed up BERTopic, we can generate the embeddings before passing it to the `Trainer`. This way, the same embeddings do not have to be generated 5 times which speeds up evaluation quite a bit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "from sentence_transformers import SentenceTransformer\n",
    "# Prepare data\n",
    "dataset_name = [\"20NewsGroup\"]\n",
    "for name in dataset_name:\n",
    "    if name == \"20NewsGroup\":\n",
    "        dataset, custom = name, False\n",
    "    else:\n",
    "        dataset, custom = name, True\n",
    "\n",
    "    data_loader = DataLoader(dataset)\n",
    "    _, timestamps = data_loader.load_docs()\n",
    "    data = data_loader.load_octis(custom)\n",
    "    data = [\" \".join(words) for words in data.get_corpus()]\n",
    "\n",
    "    # Extract embeddings\n",
    "    model = SentenceTransformer(\"all-mpnet-base-v2\")\n",
    "    # model = SentenceTransformer(\"stsb-roberta-base-v2\")\n",
    "    embeddings = model.encode(data, show_progress_bar=True)\n",
    "\n",
    "    # # Extract embeddings\n",
    "    # documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(data)]\n",
    "    # model = Doc2Vec(vector_size=300, min_count=2)\n",
    "    # embeddings = model.build_vocab(documents)\n",
    "\n",
    "    params = {\n",
    "        # \"embedding_model\": \"stsb-roberta-base-v2\",\n",
    "        \"embedding_model\": \"all-mpnet-base-v2\",\n",
    "        # \"embedding_model\": \"doc2vec\",\n",
    "        \"nr_topics\": [20, 30, 50, 75, 100],\n",
    "        \"min_topic_size\": 20,\n",
    "        \"diversity\": None,\n",
    "        \"verbose\": True\n",
    "    }\n",
    "\n",
    "    trainer = Trainer(dataset=dataset,\n",
    "                        model_name=\"BERTopic\",\n",
    "                        params=params,\n",
    "                        bt_embeddings=embeddings,\n",
    "                        custom_dataset=custom,\n",
    "                        verbose=True)\n",
    "    results = trainer.train(save=f\"./extension_results/BERTopic_{name}_mpnet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **CTM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = [\"20NewsGroup\", \"agnews\", \"reuters8\", \"StackOverflow\", \"yahoo_answer\"]\n",
    "for name in dataset_name:\n",
    "    if name == \"20NewsGroup\":\n",
    "        dataset, custom = name, False\n",
    "    else:\n",
    "        dataset, custom = name, True\n",
    "        \n",
    "    params = {\n",
    "        \"n_components\": [20, 30, 50, 75, 100],\n",
    "        \"contextual_size\":768\n",
    "    }\n",
    "\n",
    "    trainer = Trainer(dataset=dataset,\n",
    "                        model_name=\"CTM_CUSTOM\",\n",
    "                        params=params,\n",
    "                        custom_dataset=custom,\n",
    "                        verbose=True)\n",
    "    results = trainer.train(save=f\"./extension_results/CTM_Results_{dataset}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xpyBVa1Ka9Hw"
   },
   "source": [
    "## **Top2Vec**\n",
    "Aside from its Doc2Vec backend, we also want to explore its performance using the `\"all-mpnet-base-v2\"` SBERT model as that was used in BERTopic. To do so, we make a very slight change to the core code of Top2Vec, namely replacing all instances of `\"\"distiluse-base-multilingual-cased\"` with `\"all-mpnet-base-v2\"`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pc-PXbV205R3"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import strip_tags\n",
    "import umap\n",
    "import hdbscan\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from joblib import dump, load\n",
    "from sklearn.cluster import dbscan\n",
    "import tempfile\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import normalize\n",
    "from scipy.special import softmax\n",
    "from top2vec import Top2Vec\n",
    "\n",
    "try:\n",
    "    import hnswlib\n",
    "\n",
    "    _HAVE_HNSWLIB = True\n",
    "except ImportError:\n",
    "    _HAVE_HNSWLIB = False\n",
    "\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    import tensorflow_hub as hub\n",
    "    import tensorflow_text\n",
    "\n",
    "    _HAVE_TENSORFLOW = True\n",
    "except ImportError:\n",
    "    _HAVE_TENSORFLOW = False\n",
    "\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "\n",
    "    _HAVE_TORCH = True\n",
    "except ImportError:\n",
    "    _HAVE_TORCH = False\n",
    "\n",
    "logger = logging.getLogger('top2vec')\n",
    "logger.setLevel(logging.WARNING)\n",
    "sh = logging.StreamHandler()\n",
    "sh.setFormatter(logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))\n",
    "logger.addHandler(sh)\n",
    "\n",
    "\n",
    "def default_tokenizer(doc):\n",
    "    \"\"\"Tokenize documents for training and remove too long/short words\"\"\"\n",
    "    return simple_preprocess(strip_tags(doc), deacc=True)\n",
    "\n",
    "\n",
    "class Top2VecNew(Top2Vec):\n",
    "    \"\"\"\n",
    "    Top2Vec\n",
    "    Creates jointly embedded topic, document and word vectors.\n",
    "    Parameters\n",
    "    ----------\n",
    "    embedding_model: string\n",
    "        This will determine which model is used to generate the document and\n",
    "        word embeddings. The valid string options are:\n",
    "            * doc2vec\n",
    "            * universal-sentence-encoder\n",
    "            * universal-sentence-encoder-multilingual\n",
    "            * distiluse-base-multilingual-cased\n",
    "        For large data sets and data sets with very unique vocabulary doc2vec\n",
    "        could produce better results. This will train a doc2vec model from\n",
    "        scratch. This method is language agnostic. However multiple languages\n",
    "        will not be aligned.\n",
    "        Using the universal sentence encoder options will be much faster since\n",
    "        those are pre-trained and efficient models. The universal sentence\n",
    "        encoder options are suggested for smaller data sets. They are also\n",
    "        good options for large data sets that are in English or in languages\n",
    "        covered by the multilingual model. It is also suggested for data sets\n",
    "        that are multilingual.\n",
    "        For more information on universal-sentence-encoder visit:\n",
    "        https://tfhub.dev/google/universal-sentence-encoder/4\n",
    "        For more information on universal-sentence-encoder-multilingual visit:\n",
    "        https://tfhub.dev/google/universal-sentence-encoder-multilingual/3\n",
    "        The distiluse-base-multilingual-cased pre-trained sentence transformer\n",
    "        is suggested for multilingual datasets and languages that are not\n",
    "        covered by the multilingual universal sentence encoder. The\n",
    "        transformer is significantly slower than the universal sentence\n",
    "        encoder options.\n",
    "        For more informati ond istiluse-base-multilingual-cased visit:\n",
    "        https://www.sbert.net/docs/pretrained_models.html\n",
    "    embedding_model_path: string (Optional)\n",
    "        Pre-trained embedding models will be downloaded automatically by\n",
    "        default. However they can also be uploaded from a file that is in the\n",
    "        location of embedding_model_path.\n",
    "        Warning: the model at embedding_model_path must match the\n",
    "        embedding_model parameter type.\n",
    "    documents: List of str\n",
    "        Input corpus, should be a list of strings.\n",
    "    min_count: int (Optional, default 50)\n",
    "        Ignores all words with total frequency lower than this. For smaller\n",
    "        corpora a smaller min_count will be necessary.\n",
    "    speed: string (Optional, default 'learn')\n",
    "        This parameter is only used when using doc2vec as embedding_model.\n",
    "        It will determine how fast the model takes to train. The\n",
    "        fast-learn option is the fastest and will generate the lowest quality\n",
    "        vectors. The learn option will learn better quality vectors but take\n",
    "        a longer time to train. The deep-learn option will learn the best\n",
    "        quality vectors but will take significant time to train. The valid\n",
    "        string speed options are:\n",
    "        \n",
    "            * fast-learn\n",
    "            * learn\n",
    "            * deep-learn\n",
    "    use_corpus_file: bool (Optional, default False)\n",
    "        This parameter is only used when using doc2vec as embedding_model.\n",
    "        Setting use_corpus_file to True can sometimes provide speedup for\n",
    "        large datasets when multiple worker threads are available. Documents\n",
    "        are still passed to the model as a list of str, the model will create\n",
    "        a temporary corpus file for training.\n",
    "    document_ids: List of str, int (Optional)\n",
    "        A unique value per document that will be used for referring to\n",
    "        documents in search results. If ids are not given to the model, the\n",
    "        index of each document in the original corpus will become the id.\n",
    "    keep_documents: bool (Optional, default True)\n",
    "        If set to False documents will only be used for training and not saved\n",
    "        as part of the model. This will reduce model size. When using search\n",
    "        functions only document ids will be returned, not the actual\n",
    "        documents.\n",
    "    workers: int (Optional)\n",
    "        The amount of worker threads to be used in training the model. Larger\n",
    "        amount will lead to faster training.\n",
    "    \n",
    "    tokenizer: callable (Optional, default None)\n",
    "        Override the default tokenization method. If None then\n",
    "        gensim.utils.simple_preprocess will be used.\n",
    "    use_embedding_model_tokenizer: bool (Optional, default False)\n",
    "        If using an embedding model other than doc2vec, use the model's\n",
    "        tokenizer for document embedding. If set to True the tokenizer, either\n",
    "        default or passed callable will be used to tokenize the text to\n",
    "        extract the vocabulary for word embedding.\n",
    "    umap_args: dict (Optional, default None)\n",
    "        Pass custom arguments to UMAP.\n",
    "    hdbscan_args: dict (Optional, default None)\n",
    "        Pass custom arguments to HDBSCAN.\n",
    "    \n",
    "    verbose: bool (Optional, default True)\n",
    "        Whether to print status data during training.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 documents,\n",
    "                 min_count=50,\n",
    "                 embedding_model='doc2vec',\n",
    "                 embedding_model_path=None,\n",
    "                 speed='learn',\n",
    "                 use_corpus_file=False,\n",
    "                 document_ids=None,\n",
    "                 keep_documents=True,\n",
    "                 workers=None,\n",
    "                 tokenizer=None,\n",
    "                 use_embedding_model_tokenizer=False,\n",
    "                 umap_args=None,\n",
    "                 hdbscan_args=None,\n",
    "                 verbose=True\n",
    "                 ):\n",
    "\n",
    "        if verbose:\n",
    "            logger.setLevel(logging.DEBUG)\n",
    "            self.verbose = True\n",
    "        else:\n",
    "            logger.setLevel(logging.WARNING)\n",
    "            self.verbose = False\n",
    "\n",
    "        if tokenizer is None:\n",
    "            tokenizer = default_tokenizer\n",
    "\n",
    "        # validate documents\n",
    "        if not (isinstance(documents, list) or isinstance(documents, np.ndarray)):\n",
    "            raise ValueError(\"Documents need to be a list of strings\")\n",
    "        if not all((isinstance(doc, str) or isinstance(doc, np.str_)) for doc in documents):\n",
    "            raise ValueError(\"Documents need to be a list of strings\")\n",
    "        if keep_documents:\n",
    "            self.documents = np.array(documents, dtype=\"object\")\n",
    "        else:\n",
    "            self.documents = None\n",
    "\n",
    "        # validate document ids\n",
    "        if document_ids is not None:\n",
    "            if not (isinstance(document_ids, list) or isinstance(document_ids, np.ndarray)):\n",
    "                raise ValueError(\"Documents ids need to be a list of str or int\")\n",
    "\n",
    "            if len(documents) != len(document_ids):\n",
    "                raise ValueError(\"Document ids need to match number of documents\")\n",
    "            elif len(document_ids) != len(set(document_ids)):\n",
    "                raise ValueError(\"Document ids need to be unique\")\n",
    "\n",
    "            if all((isinstance(doc_id, str) or isinstance(doc_id, np.str_)) for doc_id in document_ids):\n",
    "                self.doc_id_type = np.str_\n",
    "            elif all((isinstance(doc_id, int) or isinstance(doc_id, np.int_)) for doc_id in document_ids):\n",
    "                self.doc_id_type = np.int_\n",
    "            else:\n",
    "                raise ValueError(\"Document ids need to be str or int\")\n",
    "\n",
    "            self.document_ids_provided = True\n",
    "            self.document_ids = np.array(document_ids)\n",
    "            self.doc_id2index = dict(zip(document_ids, list(range(0, len(document_ids)))))\n",
    "        else:\n",
    "            self.document_ids_provided = False\n",
    "            self.document_ids = np.array(range(0, len(documents)))\n",
    "            self.doc_id2index = dict(zip(self.document_ids, list(range(0, len(self.document_ids)))))\n",
    "            self.doc_id_type = np.int_\n",
    "\n",
    "        acceptable_embedding_models = [\"universal-sentence-encoder-multilingual\",\n",
    "                                       \"universal-sentence-encoder\",\n",
    "                                       \"all-mpnet-base-v2\"]\n",
    "\n",
    "        self.embedding_model_path = embedding_model_path\n",
    "\n",
    "        if embedding_model == 'doc2vec':\n",
    "\n",
    "            # validate training inputs\n",
    "            if speed == \"fast-learn\":\n",
    "                hs = 0\n",
    "                negative = 5\n",
    "                epochs = 40\n",
    "            elif speed == \"learn\":\n",
    "                hs = 1\n",
    "                negative = 0\n",
    "                epochs = 40\n",
    "            elif speed == \"deep-learn\":\n",
    "                hs = 1\n",
    "                negative = 0\n",
    "                epochs = 400\n",
    "            elif speed == \"test-learn\":\n",
    "                hs = 0\n",
    "                negative = 5\n",
    "                epochs = 1\n",
    "            else:\n",
    "                raise ValueError(\"speed parameter needs to be one of: fast-learn, learn or deep-learn\")\n",
    "\n",
    "            if workers is None:\n",
    "                pass\n",
    "            elif isinstance(workers, int):\n",
    "                pass\n",
    "            else:\n",
    "                raise ValueError(\"workers needs to be an int\")\n",
    "\n",
    "            doc2vec_args = {\"vector_size\": 300,\n",
    "                            \"min_count\": min_count,\n",
    "                            \"window\": 15,\n",
    "                            \"sample\": 1e-5,\n",
    "                            \"negative\": negative,\n",
    "                            \"hs\": hs,\n",
    "                            \"epochs\": epochs,\n",
    "                            \"dm\": 0,\n",
    "                            \"dbow_words\": 1}\n",
    "\n",
    "            if workers is not None:\n",
    "                doc2vec_args[\"workers\"] = workers\n",
    "\n",
    "            logger.info('Pre-processing documents for training')\n",
    "\n",
    "            if use_corpus_file:\n",
    "                processed = [' '.join(tokenizer(doc)) for doc in documents]\n",
    "                lines = \"\\n\".join(processed)\n",
    "                temp = tempfile.NamedTemporaryFile(mode='w+t')\n",
    "                temp.write(lines)\n",
    "                doc2vec_args[\"corpus_file\"] = temp.name\n",
    "\n",
    "\n",
    "            else:\n",
    "                train_corpus = [TaggedDocument(tokenizer(doc), [i]) for i, doc in enumerate(documents)]\n",
    "                doc2vec_args[\"documents\"] = train_corpus\n",
    "\n",
    "            logger.info('Creating joint document/word embedding')\n",
    "            self.embedding_model = 'doc2vec'\n",
    "            self.model = Doc2Vec(**doc2vec_args)\n",
    "\n",
    "            if use_corpus_file:\n",
    "                temp.close()\n",
    "\n",
    "        elif embedding_model in acceptable_embedding_models:\n",
    "\n",
    "            self.embed = None\n",
    "            self.embedding_model = embedding_model\n",
    "\n",
    "            self._check_import_status()\n",
    "\n",
    "            logger.info('Pre-processing documents for training')\n",
    "\n",
    "            # preprocess documents\n",
    "            tokenized_corpus = [tokenizer(doc) for doc in documents]\n",
    "\n",
    "            def return_doc(doc):\n",
    "                return doc\n",
    "\n",
    "            # preprocess vocabulary\n",
    "            vectorizer = CountVectorizer(tokenizer=return_doc, preprocessor=return_doc)\n",
    "            doc_word_counts = vectorizer.fit_transform(tokenized_corpus)\n",
    "            words = vectorizer.get_feature_names()\n",
    "            word_counts = np.array(np.sum(doc_word_counts, axis=0).tolist()[0])\n",
    "            vocab_inds = np.where(word_counts > min_count)[0]\n",
    "\n",
    "            if len(vocab_inds) == 0:\n",
    "                raise ValueError(f\"A min_count of {min_count} results in \"\n",
    "                                 f\"all words being ignored, choose a lower value.\")\n",
    "            self.vocab = [words[ind] for ind in vocab_inds]\n",
    "\n",
    "            self._check_model_status()\n",
    "\n",
    "            logger.info('Creating joint document/word embedding')\n",
    "\n",
    "            # embed words\n",
    "            self.word_indexes = dict(zip(self.vocab, range(len(self.vocab))))\n",
    "            self.word_vectors = self._l2_normalize(np.array(self.embed(self.vocab)))\n",
    "\n",
    "            # embed documents\n",
    "            if use_embedding_model_tokenizer:\n",
    "                self.document_vectors = self._embed_documents(documents)\n",
    "            else:\n",
    "                train_corpus = [' '.join(tokens) for tokens in tokenized_corpus]\n",
    "                self.document_vectors = self._embed_documents(train_corpus)\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"{embedding_model} is an invalid embedding model.\")\n",
    "\n",
    "        # create 5D embeddings of documents\n",
    "        logger.info('Creating lower dimension embedding of documents')\n",
    "\n",
    "        if umap_args is None:\n",
    "            umap_args = {'n_neighbors': 15,\n",
    "                         'n_components': 5,\n",
    "                         'metric': 'cosine'}\n",
    "\n",
    "        umap_model = umap.UMAP(**umap_args).fit(self._get_document_vectors(norm=False))\n",
    "\n",
    "        # find dense areas of document vectors\n",
    "        logger.info('Finding dense areas of documents')\n",
    "\n",
    "        if hdbscan_args is None:\n",
    "            hdbscan_args = {'min_cluster_size': 15,\n",
    "                            'metric': 'euclidean',\n",
    "                            'cluster_selection_method': 'eom'}\n",
    "\n",
    "        cluster = hdbscan.HDBSCAN(**hdbscan_args).fit(umap_model.embedding_)\n",
    "\n",
    "        # calculate topic vectors from dense areas of documents\n",
    "        logger.info('Finding topics')\n",
    "\n",
    "        # create topic vectors\n",
    "        self._create_topic_vectors(cluster.labels_)\n",
    "\n",
    "        # deduplicate topics\n",
    "        self._deduplicate_topics()\n",
    "\n",
    "        # find topic words and scores\n",
    "        self.topic_words, self.topic_word_scores = self._find_topic_words_and_scores(topic_vectors=self.topic_vectors)\n",
    "\n",
    "        # assign documents to topic\n",
    "        self.doc_top, self.doc_dist = self._calculate_documents_topic(self.topic_vectors,\n",
    "                                                                      self._get_document_vectors())\n",
    "\n",
    "        # calculate topic sizes\n",
    "        self.topic_sizes = self._calculate_topic_sizes(hierarchy=False)\n",
    "\n",
    "        # re-order topics\n",
    "        self._reorder_topics(hierarchy=False)\n",
    "\n",
    "        # initialize variables for hierarchical topic reduction\n",
    "        self.topic_vectors_reduced = None\n",
    "        self.doc_top_reduced = None\n",
    "        self.doc_dist_reduced = None\n",
    "        self.topic_sizes_reduced = None\n",
    "        self.topic_words_reduced = None\n",
    "        self.topic_word_scores_reduced = None\n",
    "        self.hierarchy = None\n",
    "\n",
    "        # initialize document indexing variables\n",
    "        self.document_index = None\n",
    "        self.serialized_document_index = None\n",
    "        self.documents_indexed = False\n",
    "        self.index_id2doc_id = None\n",
    "        self.doc_id2index_id = None\n",
    "\n",
    "        # initialize word indexing variables\n",
    "        self.word_index = None\n",
    "        self.serialized_word_index = None\n",
    "        self.words_indexed = False\n",
    "\n",
    "    def _check_import_status(self):\n",
    "        if self.embedding_model != 'all-mpnet-base-v2':\n",
    "            if not _HAVE_TENSORFLOW:\n",
    "                raise ImportError(f\"{self.embedding_model} is not available.\\n\\n\"\n",
    "                                  \"Try: pip install top2vec[sentence_encoders]\\n\\n\"\n",
    "                                  \"Alternatively try: pip install tensorflow tensorflow_hub tensorflow_text\")\n",
    "        else:\n",
    "            if not _HAVE_TORCH:\n",
    "                raise ImportError(f\"{self.embedding_model} is not available.\\n\\n\"\n",
    "                                  \"Try: pip install top2vec[sentence_transformers]\\n\\n\"\n",
    "                                  \"Alternatively try: pip install torch sentence_transformers\")\n",
    "\n",
    "    def _check_model_status(self):\n",
    "        if self.embed is None:\n",
    "            if self.verbose is False:\n",
    "                logger.setLevel(logging.DEBUG)\n",
    "\n",
    "            if self.embedding_model != \"all-mpnet-base-v2\":\n",
    "                if self.embedding_model_path is None:\n",
    "                    logger.info(f'Downloading {self.embedding_model} model')\n",
    "                    if self.embedding_model == \"universal-sentence-encoder-multilingual\":\n",
    "                        module = \"https://tfhub.dev/google/universal-sentence-encoder-multilingual/3\"\n",
    "                    else:\n",
    "                        module = \"https://tfhub.dev/google/universal-sentence-encoder/4\"\n",
    "                else:\n",
    "                    logger.info(f'Loading {self.embedding_model} model at {self.embedding_model_path}')\n",
    "                    module = self.embedding_model_path\n",
    "                self.embed = hub.load(module)\n",
    "\n",
    "            else:\n",
    "                if self.embedding_model_path is None:\n",
    "                    logger.info(f'Downloading {self.embedding_model} model')\n",
    "                    module = 'all-mpnet-base-v2'\n",
    "                else:\n",
    "                    logger.info(f'Loading {self.embedding_model} model at {self.embedding_model_path}')\n",
    "                    module = self.embedding_model_path\n",
    "                model = SentenceTransformer(module)\n",
    "                self.embed = model.encode\n",
    "\n",
    "        if self.verbose is False:\n",
    "            logger.setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oz16gmSjHcs6"
   },
   "source": [
    "We can then use this `Top2VecNew` class to run our experiments including the `\"all-mpnet-base-v2\"` model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in [\"20NewsGroup\", \"reuters8\"]:\n",
    "    if name == \"20NewsGroup\":\n",
    "        dataset, custom = name, False\n",
    "    else:\n",
    "        dataset, custom = name, True\n",
    "        \n",
    "    params = {\"nr_topics\": [20, 30, 50, 75, 100], \"embedding_model\": \"all-mpnet-base-v2\", \"min_count\": 20,\n",
    "                \"speed\": \"deep-learn\",\n",
    "                \"hdbscan_args\": {'min_cluster_size': 5,\n",
    "                                'metric': 'euclidean',\n",
    "                                'cluster_selection_method': 'eom'}}\n",
    "\n",
    "    trainer = Trainer(dataset=dataset,\n",
    "                        custom_dataset=custom,\n",
    "                        custom_model=Top2VecNew,\n",
    "                        model_name=\"Top2Vec\",\n",
    "                        params=params,\n",
    "                        verbose=True)\n",
    "    results = trainer.train(save=f\"./extension_results/Top2Vec_{name}_mpnet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TSCTM ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = [\"20NewsGroup\", \"agnews\", \"reuters8\", \"StackOverflow\", \"yahoo_answer\"]\n",
    "for i, random_state in enumerate([0]):\n",
    "    for name in dataset_name:\n",
    "        if name == \"20NewsGroup\":\n",
    "            dataset, custom = name, False\n",
    "        else:\n",
    "            dataset, custom = name, True\n",
    "\n",
    "        params =  {\"num_topics\": [20], \"random_seed\": random_state}\n",
    "        trainer = Trainer(dataset=dataset,\n",
    "                        model_name=\"tsctm\",\n",
    "                        params=params,\n",
    "                        custom_dataset=custom,\n",
    "                        verbose=True)\n",
    "        results = trainer.train(save=f\"./extension_results/tsctm_{name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bertop (MPNET) + LDA ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "import numpy as np\n",
    "\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield (gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
    "\n",
    "def get_vec_lda(model, corpus, k):\n",
    "    \"\"\"\n",
    "    Get the LDA vector representation (probabilistic topic assignments for all documents)\n",
    "    :return: vec_lda with dimension: (n_doc * n_topic)\n",
    "    \"\"\"\n",
    "    # n_doc = len(corpus)\n",
    "    # vec_lda = np.zeros((n_doc, k))\n",
    "    # for i in range(n_doc):\n",
    "    #     # get the distribution for the i-th document in corpus\n",
    "    #     for topic, prob in model.get_document_topics(corpus[i]):\n",
    "    #         vec_lda[i, topic] = prob\n",
    "\n",
    "    n_doc = len(corpus)\n",
    "    vec_lda = np.zeros((n_doc, k))\n",
    "    for i, aa in enumerate(model.load_document_topics()):\n",
    "        for item in aa:\n",
    "            vec_lda[i, item[0]] = item[1]\n",
    "\n",
    "    return vec_lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Prepare data\n",
    "dataset_name = [\"reuters8\"]\n",
    "for name in dataset_name:\n",
    "    if name == \"20NewsGroup\":\n",
    "        dataset, custom = name, False\n",
    "    else:\n",
    "        dataset, custom = name, True\n",
    "        \n",
    "    data_loader = DataLoader(dataset)\n",
    "    _, timestamps = data_loader.load_docs()\n",
    "    sentences = data_loader.load_octis(custom)\n",
    "    sentences = [\" \".join(words) for words in sentences.get_corpus()]\n",
    "\n",
    "    data_words = list(sent_to_words(sentences))\n",
    "    # turn tokenized documents into a id <-> term dictionary\n",
    "    id2word = corpora.Dictionary(data_words)\n",
    "    # convert tokenized documents into a document-term matrix\n",
    "    corpus = [id2word.doc2bow(text) for text in data_words]\n",
    "\n",
    "    gamma = 30\n",
    "\n",
    "    for ktop in [20, 30, 50, 75, 100]:\n",
    "        print('Getting vector representations for LDA ...')\n",
    "\n",
    "        mallet_path = '/mnt/raid1/gzhou/mallet-2.0.8/bin/mallet'\n",
    "        ldamodel = gensim.models.wrappers.LdaMallet(mallet_path=mallet_path, corpus=corpus, \n",
    "                                                            num_topics=ktop, id2word=id2word, random_seed=42)\n",
    "\n",
    "        # ldamodel = gensim.models.ldamodel.LdaModel(corpus=corpus, num_topics=ktop, id2word=id2word,\n",
    "        #                                                 passes=20)\n",
    "        vec_lda = get_vec_lda(ldamodel, corpus, ktop)\n",
    "        print('Getting vector representations for LDA. Done!')\n",
    "\n",
    "\n",
    "        print('Getting vector representations for BERT ...')\n",
    "        # Extract embeddings\n",
    "        model = SentenceTransformer(\"all-mpnet-base-v2\")\n",
    "        # model = SentenceTransformer(\"stsb-roberta-base-v2\")\n",
    "        vec_bert = np.array(model.encode(sentences, show_progress_bar=True))\n",
    "        # embeddings = model.encode(sentences, show_progress_bar=True)\n",
    "        print('Getting vector representations for BERT. Done!')  \n",
    "\n",
    "\n",
    "        print(f'vec_lda.shape is {vec_lda.shape}')\n",
    "        print(f'vec_bert.shape is {vec_bert.shape}')\n",
    "        vec_ldabert = np.c_[vec_lda * gamma, vec_bert]\n",
    "        print(f'vec_ldabert.shape is {vec_ldabert.shape}')\n",
    "\n",
    "        params = {\n",
    "            # \"embedding_model\": \"stsb-roberta-base-v2\",\n",
    "            \"embedding_model\": \"all-mpnet-base-v2\",\n",
    "            # \"embedding_model\": \"doc2vec\",\n",
    "            \"nr_topics\": ktop,\n",
    "            \"min_topic_size\": 20,\n",
    "            \"diversity\": None,\n",
    "            \"verbose\": True\n",
    "        }\n",
    "\n",
    "        trainer = Trainer(dataset=dataset,\n",
    "                            model_name=\"BERTopic\",\n",
    "                            params=params,\n",
    "                            bt_embeddings=vec_ldabert,\n",
    "                            custom_dataset=custom,\n",
    "                            verbose=True)\n",
    "                            \n",
    "        results = trainer.train(save=f\"./extension_results/BERTopic+LDA_{name}_mpnet_{gamma}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "BERTopic OCTIS v3.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3.7.6 ('tmeva')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "afa201d01b70fa10cb9054bab608e3c6f9450b124856fdd2194cf0d90fbbf2f6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
