{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iCGERDM6UKd_"
   },
   "source": [
    "# **Topic Model Evaluation**\n",
    "Here, you will find the code needed to run the experiments of the paper:\n",
    "\n",
    "*BERTopic: Neural topic modeling with a class-based TF-IDF procedure*.\n",
    "\n",
    "The package itself can be found [here](https://github.com/MaartenGr/BERTopic) and the repository for evaluation [here]()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MFwg78cJ0BFg"
   },
   "source": [
    "## **Installation**\n",
    "First, we need to install a few packages in order to run our experiments. Most of the packages are installed through the `tm_evaluation` package of which [OCTIS](https://github.com/MIND-Lab/OCTIS) is an important component. \n",
    "\n",
    "You can install the evaluation package with `pip install .` from the root. To additionally install CTM run `pip install .[ctm]`To install BERTopic, run `pip install bertopic==v0.9.4` after installing the base package or use `pip install .[bertopic]`. Top2Vec should be installed with `pip install top2vec==v1.0.26` after installing the base package. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dLesRC-17zN9"
   },
   "source": [
    "To run a faster version of LDAseq for dynamic topic modeling, we need to uninstall gensim and install a specific merge that allows for this speed-up. First, run `pip uninstall gensim -y`, then, run `pip install git+https://github.com/RaRe-Technologies/gensim.git@refs/pull/3172/merge`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fisNVVa977e-"
   },
   "source": [
    "**NOTE**: After installing the above packages, make sure to restart the runtime otherwise you are likely to run into issues. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nQnAdKJ-Do1S"
   },
   "source": [
    "#  1. **Data**\n",
    "Some of the data can be accessed through OCTIS, such as the `20NewsGroup` and `BBC_News` datasets. Other datasets, however, are downloaded and then run through OCTIS in order to be used in their pipeline. \n",
    "\n",
    "The datasets that we are going to be preparing are: \n",
    "* Trump's tweets\n",
    "* United Nations general debates between 2006 and 2015 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from evaluation import Trainer, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GRJJvgHyjYa6"
   },
   "source": [
    "# 2. **Evaluation**\n",
    "After preparing our data, we can start evaluating the topic models as used in the experiments. OCTIS already has a number of models prepared that we can use directly as shown below. \n",
    "\n",
    "First, we specify what the dataset is and whether that was a custom dataset not found in OCTIS. To run our custom trump dataset, we run `dataset, custom = \"trump\", True`. In contrast, if we are to use the prepackaged 20NewsGroup dataset, we run `dataset, custom = \"20NewsGroup\", False` instead. \n",
    "\n",
    "The OCTIS datasets can be found [here](https://github.com/MIND-Lab/OCTIS#available-datasets). \n",
    "\n",
    "Second, we define a number of parameters to be used for the model. It uses the following format: \n",
    "\n",
    "`params = {\"num_topics\": [(i+1)*10 for i in range(5)], \"random_state\": random_state}`\n",
    "\n",
    "were we define a number of topics to loop over and calculate the evluation metrics but also define a number of parameters used in the models. \n",
    "\n",
    "#### **Parameters**\n",
    "The parameters for LDA and NMF:\n",
    "\n",
    "\n",
    "```python\n",
    "params = {\"num_topics\": [(i+1)*10 for i in range(5)], \"random_state\": random_state}`\n",
    "```\n",
    "\n",
    "The parameters for Top2Vec:\n",
    "\n",
    "```python\n",
    "params = {\"nr_topics\": [(i+1)*10 for i in range(5)],\n",
    "          \"hdbscan_args\": {'min_cluster_size': 15,\n",
    "                            'metric': 'euclidean',\n",
    "                            'cluster_selection_method': 'eom'}}\n",
    "```\n",
    "Note that the `min_cluster_size` is 15 for all datasets except BBC_News.\n",
    "\n",
    "The parameters for CTM:\n",
    "\n",
    "```python\n",
    "params = {\n",
    "    \"n_components\": [(i+1)*10 for i in range(5)],\n",
    "    \"contextual_size\":768\n",
    "}\n",
    "```\n",
    "\n",
    "The parameters for BERTopic:\n",
    "\n",
    "```python\n",
    "params = {\n",
    "    \"nr_topics\": [(i+1)*10 for i in range(5)],\n",
    "    \"min_topic_size\": 15,\n",
    "    \"verbose\": True\n",
    "}\n",
    "```\n",
    "\n",
    "Note that the `min_topic_size` is 15 for all datasets except BBC_News. Note that we do not set a `embedding_model` here. We do this on purpose as we can generate the embeddings beforehand and pass those to BERTopic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/joe/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from evaluation import Trainer, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l8s9zjC5A6JD"
   },
   "source": [
    "#### LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "uAZ_ckYBeFDY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic Number is 10\n",
      "TOPIC 1 are: ['city', 'center', 'capital', 'century', 'founded', 'located', 'population', 'university', 'river', 'port']\n",
      "TOPIC 2 are: ['de', 'life', 'french', 'eng', 'english', 'king', 'novel', 'john', 'death', 'england']\n"
     ]
    }
   ],
   "source": [
    "for i, random_state in enumerate([42]):\n",
    "    # dataset, custom = \"20NewsGroup\", False\n",
    "    dataset, custom = \"/home/joe/topic-modeling/processed-datasets/GRL\", True\n",
    "    params = {\"num_topics\": [20, 30], \"random_state\": random_state}\n",
    "\n",
    "    trainer = Trainer(dataset=dataset,\n",
    "                      model_name=\"LDA\",\n",
    "                      params=params,\n",
    "                      custom_dataset=custom,\n",
    "                      verbose=True)\n",
    "    results = trainer.train(save=f\"LDA_GRL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CGhfpkBZGtRl"
   },
   "source": [
    "## **BERTopic**\n",
    "\n",
    "To speed up BERTopic, we can generate the embeddings before passing it to the `Trainer`. This way, the same embeddings do not have to be generated 5 times which speeds up evaluation quite a bit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "XhFXGK8oxVpi"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Prepare data\n",
    "dataset, custom = \"20NewsGroup\", False\n",
    "data_loader = DataLoader(dataset)\n",
    "_, timestamps = data_loader.load_docs()\n",
    "data = data_loader.load_octis(custom)\n",
    "data = [\" \".join(words) for words in data.get_corpus()]\n",
    "\n",
    "# Extract embeddings\n",
    "model = SentenceTransformer(\"all-mpnet-base-v2\")\n",
    "embeddings = model.encode(data, show_progress_bar=True)\n",
    "\n",
    "# # Extract embeddings\n",
    "# documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(data)]\n",
    "# model = Doc2Vec(vector_size=300, min_count=2)\n",
    "# embeddings = model.build_vocab(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nb1afGtJEuw0"
   },
   "source": [
    "As show above, we load in the `data` which the data loader and combine the tokens in each document to generate our training data. Then, we pass it to the sentence transformer model of our choice and generate the embeddings. \n",
    "\n",
    "Next, we pass these embeddings to the `bt_embeddings` parameter to speed up training: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "II0a3WJP37V0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-22 03:18:25,069 - BERTopic - Reduced dimensionality with UMAP\n",
      "2022-04-22 03:18:25,497 - BERTopic - Clustered UMAP embeddings with HDBSCAN\n",
      "2022-04-22 03:18:27,106 - BERTopic - Reduced number of topics from 83 to 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "========================================================================================================================\n",
      "npmi: 0.18140171560355497\n",
      "diversity: 0.9466666666666667\n",
      "cv: 0.7495090676822437\n",
      "uci: 1.3705012367128182\n",
      " \n",
      "Topic 1 cotains words: ['game', 'team', 'play', 'player', 'win', 'year', 'season', 'score', 'good', 'hit']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 2 cotains words: ['car', 'bike', 'ride', 'engine', 'mile', 'road', 'tire', 'drive', 'good', 'front']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 3 cotains words: ['church', 'homosexual', 'sin', 'man', 'people', 'christian', 'word', 'scripture', 'faith', 'love']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 4 cotains words: ['armenian', 'turkish', 'people', 'israeli', 'kill', 'jewish', 'village', 'genocide', 'arab', 'soldier']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 5 cotains words: ['key', 'encryption', 'chip', 'clipper', 'privacy', 'government', 'security', 'message', 'phone', 'escrow']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 6 cotains words: ['drug', 'patient', 'doctor', 'disease', 'medical', 'treatment', 'food', 'study', 'effect', 'health']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 7 cotains words: ['space', 'launch', 'orbit', 'mission', 'satellite', 'earth', 'shuttle', 'solar', 'planet', 'moon']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 8 cotains words: ['card', 'monitor', 'driver', 'video', 'mouse', 'vga', 'mode', 'window', 'screen', 'color']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 9 cotains words: ['drive', 'scsi', 'disk', 'ide', 'controller', 'hard', 'floppy', 'bus', 'bio', 'boot']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 10 cotains words: ['people', 'president', 'government', 'make', 'state', 'job', 'work', 'law', 'decision', 'talk']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 11 cotains words: ['fire', 'batf', 'gas', 'warrant', 'agent', 'cop', 'police', 'trial', 'evidence', 'compound']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 12 cotains words: ['mail', 'list', 'address', 'request', 'send', 'post', 'email', 'group', 'message', 'mailing']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 13 cotains words: ['atheist', 'atheism', 'belief', 'moral', 'argument', 'science', 'evidence', 'exist', 'morality', 'religion']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 14 cotains words: ['sale', 'offer', 'sell', 'price', 'include', 'interested', 'condition', 'manual', 'shipping', 'package']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 15 cotains words: ['gun', 'firearm', 'weapon', 'crime', 'file', 'control', 'law', 'criminal', 'bill', 'rate']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-22 03:18:53,316 - BERTopic - Reduced dimensionality with UMAP\n",
      "2022-04-22 03:18:53,746 - BERTopic - Clustered UMAP embeddings with HDBSCAN\n",
      "2022-04-22 03:18:55,424 - BERTopic - Reduced number of topics from 90 to 21\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "========================================================================================================================\n",
      "npmi: 0.1799843687154114\n",
      "diversity: 0.905\n",
      "cv: 0.7290879168538626\n",
      "uci: 1.4156813916738824\n",
      " \n",
      "Topic 1 cotains words: ['game', 'team', 'play', 'player', 'win', 'year', 'season', 'score', 'hit', 'goal']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 2 cotains words: ['car', 'bike', 'ride', 'engine', 'mile', 'road', 'tire', 'drive', 'buy', 'front']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 3 cotains words: ['drug', 'patient', 'doctor', 'medical', 'health', 'disease', 'treatment', 'study', 'food', 'effect']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 4 cotains words: ['key', 'encryption', 'chip', 'clipper', 'government', 'escrow', 'phone', 'algorithm', 'bit', 'encrypt']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 5 cotains words: ['space', 'launch', 'orbit', 'satellite', 'mission', 'earth', 'shuttle', 'solar', 'planet', 'moon']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 6 cotains words: ['church', 'sin', 'scripture', 'faith', 'word', 'christian', 'man', 'love', 'life', 'son']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 7 cotains words: ['drive', 'scsi', 'disk', 'ide', 'controller', 'hard', 'floppy', 'bus', 'bio', 'boot']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 8 cotains words: ['president', 'people', 'government', 'make', 'moral', 'job', 'state', 'decision', 'work', 'question']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 9 cotains words: ['battery', 'sale', 'offer', 'sell', 'condition', 'interested', 'price', 'shipping', 'include', 'good']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 10 cotains words: ['server', 'window', 'motif', 'client', 'run', 'problem', 'program', 'file', 'application', 'error']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 11 cotains words: ['sound', 'ground', 'audio', 'circuit', 'voltage', 'signal', 'output', 'current', 'radio', 'frequency']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 12 cotains words: ['fire', 'batf', 'gas', 'warrant', 'cop', 'tear', 'police', 'evidence', 'agent', 'flame']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 13 cotains words: ['monitor', 'card', 'vga', 'video', 'connector', 'slot', 'screen', 'pin', 'board', 'apple']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 14 cotains words: ['gun', 'firearm', 'weapon', 'crime', 'file', 'control', 'law', 'criminal', 'bill', 'carry']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 15 cotains words: ['israeli', 'arab', 'jewish', 'peace', 'state', 'territory', 'attack', 'war', 'country', 'land']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 16 cotains words: ['atheist', 'atheism', 'belief', 'argument', 'evidence', 'exist', 'science', 'existence', 'religion', 'religious']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 17 cotains words: ['font', 'printer', 'print', 'laser', 'window', 'driver', 'character', 'problem', 'paper', 'file']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 18 cotains words: ['homosexual', 'homosexuality', 'gay', 'sex', 'man', 'sexual', 'male', 'people', 'sin', 'church']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 19 cotains words: ['armenian', 'turkish', 'people', 'genocide', 'greek', 'kill', 'village', 'russian', 'massacre', 'muslim']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 20 cotains words: ['driver', 'mouse', 'card', 'window', 'video', 'mode', 'problem', 'screen', 'color', 'version']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-22 03:19:21,693 - BERTopic - Reduced dimensionality with UMAP\n",
      "2022-04-22 03:19:22,116 - BERTopic - Clustered UMAP embeddings with HDBSCAN\n",
      "2022-04-22 03:19:23,868 - BERTopic - Reduced number of topics from 96 to 26\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "========================================================================================================================\n",
      "npmi: 0.1757876172211691\n",
      "diversity: 0.868\n",
      "cv: 0.714601565539109\n",
      "uci: 1.376805622072994\n",
      " \n",
      "Topic 1 cotains words: ['game', 'team', 'play', 'player', 'win', 'season', 'year', 'score', 'hit', 'goal']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 2 cotains words: ['car', 'bike', 'ride', 'engine', 'mile', 'tire', 'oil', 'dealer', 'road', 'front']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 3 cotains words: ['drug', 'patient', 'doctor', 'medical', 'health', 'disease', 'treatment', 'food', 'study', 'effect']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 4 cotains words: ['key', 'encryption', 'chip', 'clipper', 'government', 'escrow', 'phone', 'algorithm', 'encrypt', 'bit']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 5 cotains words: ['drive', 'scsi', 'disk', 'ide', 'controller', 'hard', 'floppy', 'boot', 'bio', 'system']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 6 cotains words: ['church', 'sin', 'faith', 'scripture', 'christian', 'word', 'love', 'man', 'life', 'son']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 7 cotains words: ['president', 'people', 'government', 'make', 'moral', 'job', 'state', 'decision', 'question', 'work']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 8 cotains words: ['space', 'launch', 'orbit', 'mission', 'satellite', 'shuttle', 'earth', 'solar', 'moon', 'planet']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 9 cotains words: ['gun', 'firearm', 'weapon', 'crime', 'control', 'file', 'law', 'criminal', 'carry', 'bill']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 10 cotains words: ['file', 'entry', 'font', 'program', 'server', 'output', 'version', 'include', 'widget', 'system']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 11 cotains words: ['sale', 'offer', 'sell', 'condition', 'interested', 'price', 'good', 'pay', 'tape', 'shipping']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 12 cotains words: ['monitor', 'card', 'video', 'vga', 'screen', 'slot', 'apple', 'resolution', 'color', 'buy']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 13 cotains words: ['israeli', 'arab', 'jewish', 'peace', 'territory', 'state', 'land', 'country', 'village', 'occupy']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 14 cotains words: ['sound', 'audio', 'circuit', 'frequency', 'output', 'input', 'voltage', 'radio', 'signal', 'noise']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 15 cotains words: ['homosexual', 'homosexuality', 'gay', 'sex', 'sexual', 'man', 'male', 'people', 'sin', 'church']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 16 cotains words: ['mail', 'email', 'address', 'list', 'post', 'send', 'reply', 'mailing', 'advance', 'response']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 17 cotains words: ['armenian', 'turkish', 'genocide', 'people', 'greek', 'kill', 'village', 'russian', 'massacre', 'muslim']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 18 cotains words: ['driver', 'mouse', 'card', 'window', 'mode', 'video', 'problem', 'color', 'screen', 'graphic']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 19 cotains words: ['article', 'post', 'newsgroup', 'group', 'read', 'context', 'make', 'people', 'time', 'quote']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 20 cotains words: ['modem', 'port', 'fax', 'serial', 'connect', 'pin', 'software', 'work', 'problem', 'cable']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 21 cotains words: ['atheist', 'atheism', 'belief', 'argument', 'religion', 'exist', 'existence', 'religious', 'evidence', 'true']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 22 cotains words: ['image', 'color', 'file', 'format', 'convert', 'quality', 'bit', 'version', 'display', 'program']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 23 cotains words: ['motif', 'run', 'window', 'problem', 'server', 'crash', 'error', 'application', 'machine', 'display']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 24 cotains words: ['printer', 'print', 'laser', 'driver', 'paper', 'page', 'problem', 'sheet', 'quality', 'window']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 25 cotains words: ['batf', 'warrant', 'agent', 'cop', 'trial', 'police', 'federal', 'compound', 'arrest', 'charge']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-22 03:19:53,421 - BERTopic - Reduced dimensionality with UMAP\n",
      "2022-04-22 03:19:53,876 - BERTopic - Clustered UMAP embeddings with HDBSCAN\n",
      "2022-04-22 03:19:55,881 - BERTopic - Reduced number of topics from 87 to 31\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "========================================================================================================================\n",
      "npmi: 0.17511031585436976\n",
      "diversity: 0.8433333333333334\n",
      "cv: 0.7072958434559364\n",
      "uci: 1.4072931367140673\n",
      " \n",
      "Topic 1 cotains words: ['game', 'team', 'play', 'player', 'win', 'season', 'year', 'score', 'hit', 'goal']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 2 cotains words: ['drug', 'patient', 'doctor', 'medical', 'treatment', 'health', 'disease', 'food', 'study', 'effect']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 3 cotains words: ['key', 'encryption', 'chip', 'clipper', 'government', 'escrow', 'phone', 'algorithm', 'encrypt', 'secure']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 4 cotains words: ['church', 'faith', 'sin', 'scripture', 'christian', 'word', 'man', 'love', 'life', 'people']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 5 cotains words: ['mhz', 'chip', 'motherboard', 'cpu', 'card', 'board', 'speed', 'ram', 'fan', 'apple']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 6 cotains words: ['drive', 'scsi', 'disk', 'ide', 'controller', 'hard', 'floppy', 'bio', 'boot', 'bus']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 7 cotains words: ['space', 'launch', 'orbit', 'mission', 'satellite', 'shuttle', 'earth', 'solar', 'moon', 'planet']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 8 cotains words: ['bike', 'ride', 'dog', 'motorcycle', 'road', 'turn', 'front', 'back', 'rear', 'mile']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 9 cotains words: ['car', 'engine', 'oil', 'dealer', 'mile', 'tire', 'buy', 'drive', 'transmission', 'model']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 10 cotains words: ['sound', 'ground', 'audio', 'circuit', 'voltage', 'current', 'wire', 'frequency', 'signal', 'input']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 11 cotains words: ['moral', 'government', 'people', 'morality', 'society', 'objective', 'freedom', 'make', 'law', 'state']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 12 cotains words: ['gun', 'firearm', 'weapon', 'crime', 'control', 'file', 'law', 'criminal', 'bill', 'rate']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 13 cotains words: ['israeli', 'arab', 'jewish', 'state', 'territory', 'peace', 'land', 'occupy', 'country', 'policy']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 14 cotains words: ['sale', 'offer', 'sell', 'interested', 'condition', 'price', 'ticket', 'pay', 'speaker', 'include']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 15 cotains words: ['printer', 'font', 'print', 'laser', 'window', 'driver', 'character', 'problem', 'paper', 'page']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 16 cotains words: ['monitor', 'card', 'vga', 'video', 'screen', 'apple', 'color', 'buy', 'resolution', 'inch']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 17 cotains words: ['homosexual', 'homosexuality', 'gay', 'sex', 'man', 'sexual', 'male', 'people', 'sin', 'church']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 18 cotains words: ['armenian', 'turkish', 'genocide', 'people', 'greek', 'kill', 'village', 'russian', 'massacre', 'muslim']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 19 cotains words: ['batf', 'warrant', 'cop', 'agent', 'trial', 'evidence', 'police', 'court', 'arrest', 'fire']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 20 cotains words: ['driver', 'mouse', 'card', 'window', 'mode', 'video', 'problem', 'color', 'screen', 'vga']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 21 cotains words: ['mail', 'email', 'address', 'list', 'post', 'send', 'mailing', 'reply', 'advance', 'response']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 22 cotains words: ['motif', 'window', 'run', 'server', 'client', 'problem', 'program', 'memory', 'application', 'machine']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 23 cotains words: ['file', 'entry', 'output', 'program', 'include', 'version', 'widget', 'server', 'build', 'source']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 24 cotains words: ['image', 'color', 'file', 'format', 'quality', 'convert', 'bit', 'display', 'version', 'program']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 25 cotains words: ['post', 'group', 'newsgroup', 'article', 'read', 'people', 'posting', 'news', 'make', 'net']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 26 cotains words: ['modem', 'port', 'fax', 'serial', 'connect', 'pin', 'software', 'work', 'problem', 'cable']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 27 cotains words: ['atheist', 'atheism', 'belief', 'religion', 'argument', 'religious', 'exist', 'existence', 'faith', 'christian']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 28 cotains words: ['science', 'evidence', 'exist', 'theory', 'scientific', 'observation', 'existence', 'claim', 'argument', 'reason']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 29 cotains words: ['fire', 'gas', 'tear', 'flame', 'smoke', 'tank', 'start', 'building', 'burn', 'make']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 30 cotains words: ['point', 'line', 'edge', 'image', 'plane', 'algorithm', 'surface', 'circle', 'graphic', 'draw']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      " \n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    \"embedding_model\": \"all-mpnet-base-v2\",\n",
    "    # \"embedding_model\": \"doc2vec\",\n",
    "    \"nr_topics\": [15, 20, 25, 30],\n",
    "    \"min_topic_size\": 15,\n",
    "    \n",
    "    \"diversity\": None,\n",
    "    \"verbose\": True\n",
    "}\n",
    "\n",
    "trainer = Trainer(dataset=dataset,\n",
    "                    model_name=\"BERTopic\",\n",
    "                    params=params,\n",
    "                    bt_embeddings=embeddings,\n",
    "                    custom_dataset=custom,\n",
    "                    verbose=True)\n",
    "results = trainer.train(save=f\"BERTopic_20news_mpnet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xpyBVa1Ka9Hw"
   },
   "source": [
    "## **Top2Vec**\n",
    "Aside from its Doc2Vec backend, we also want to explore its performance using the `\"all-mpnet-base-v2\"` SBERT model as that was used in BERTopic. To do so, we make a very slight change to the core code of Top2Vec, namely replacing all instances of `\"\"distiluse-base-multilingual-cased\"` with `\"all-mpnet-base-v2\"`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Pc-PXbV205R3"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import strip_tags\n",
    "import umap\n",
    "import hdbscan\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from joblib import dump, load\n",
    "from sklearn.cluster import dbscan\n",
    "import tempfile\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import normalize\n",
    "from scipy.special import softmax\n",
    "from top2vec import Top2Vec\n",
    "\n",
    "try:\n",
    "    import hnswlib\n",
    "\n",
    "    _HAVE_HNSWLIB = True\n",
    "except ImportError:\n",
    "    _HAVE_HNSWLIB = False\n",
    "\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    import tensorflow_hub as hub\n",
    "    import tensorflow_text\n",
    "\n",
    "    _HAVE_TENSORFLOW = True\n",
    "except ImportError:\n",
    "    _HAVE_TENSORFLOW = False\n",
    "\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "\n",
    "    _HAVE_TORCH = True\n",
    "except ImportError:\n",
    "    _HAVE_TORCH = False\n",
    "\n",
    "logger = logging.getLogger('top2vec')\n",
    "logger.setLevel(logging.WARNING)\n",
    "sh = logging.StreamHandler()\n",
    "sh.setFormatter(logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))\n",
    "logger.addHandler(sh)\n",
    "\n",
    "\n",
    "def default_tokenizer(doc):\n",
    "    \"\"\"Tokenize documents for training and remove too long/short words\"\"\"\n",
    "    return simple_preprocess(strip_tags(doc), deacc=True)\n",
    "\n",
    "\n",
    "class Top2VecNew(Top2Vec):\n",
    "    \"\"\"\n",
    "    Top2Vec\n",
    "    Creates jointly embedded topic, document and word vectors.\n",
    "    Parameters\n",
    "    ----------\n",
    "    embedding_model: string\n",
    "        This will determine which model is used to generate the document and\n",
    "        word embeddings. The valid string options are:\n",
    "            * doc2vec\n",
    "            * universal-sentence-encoder\n",
    "            * universal-sentence-encoder-multilingual\n",
    "            * distiluse-base-multilingual-cased\n",
    "        For large data sets and data sets with very unique vocabulary doc2vec\n",
    "        could produce better results. This will train a doc2vec model from\n",
    "        scratch. This method is language agnostic. However multiple languages\n",
    "        will not be aligned.\n",
    "        Using the universal sentence encoder options will be much faster since\n",
    "        those are pre-trained and efficient models. The universal sentence\n",
    "        encoder options are suggested for smaller data sets. They are also\n",
    "        good options for large data sets that are in English or in languages\n",
    "        covered by the multilingual model. It is also suggested for data sets\n",
    "        that are multilingual.\n",
    "        For more information on universal-sentence-encoder visit:\n",
    "        https://tfhub.dev/google/universal-sentence-encoder/4\n",
    "        For more information on universal-sentence-encoder-multilingual visit:\n",
    "        https://tfhub.dev/google/universal-sentence-encoder-multilingual/3\n",
    "        The distiluse-base-multilingual-cased pre-trained sentence transformer\n",
    "        is suggested for multilingual datasets and languages that are not\n",
    "        covered by the multilingual universal sentence encoder. The\n",
    "        transformer is significantly slower than the universal sentence\n",
    "        encoder options.\n",
    "        For more informati ond istiluse-base-multilingual-cased visit:\n",
    "        https://www.sbert.net/docs/pretrained_models.html\n",
    "    embedding_model_path: string (Optional)\n",
    "        Pre-trained embedding models will be downloaded automatically by\n",
    "        default. However they can also be uploaded from a file that is in the\n",
    "        location of embedding_model_path.\n",
    "        Warning: the model at embedding_model_path must match the\n",
    "        embedding_model parameter type.\n",
    "    documents: List of str\n",
    "        Input corpus, should be a list of strings.\n",
    "    min_count: int (Optional, default 50)\n",
    "        Ignores all words with total frequency lower than this. For smaller\n",
    "        corpora a smaller min_count will be necessary.\n",
    "    speed: string (Optional, default 'learn')\n",
    "        This parameter is only used when using doc2vec as embedding_model.\n",
    "        It will determine how fast the model takes to train. The\n",
    "        fast-learn option is the fastest and will generate the lowest quality\n",
    "        vectors. The learn option will learn better quality vectors but take\n",
    "        a longer time to train. The deep-learn option will learn the best\n",
    "        quality vectors but will take significant time to train. The valid\n",
    "        string speed options are:\n",
    "        \n",
    "            * fast-learn\n",
    "            * learn\n",
    "            * deep-learn\n",
    "    use_corpus_file: bool (Optional, default False)\n",
    "        This parameter is only used when using doc2vec as embedding_model.\n",
    "        Setting use_corpus_file to True can sometimes provide speedup for\n",
    "        large datasets when multiple worker threads are available. Documents\n",
    "        are still passed to the model as a list of str, the model will create\n",
    "        a temporary corpus file for training.\n",
    "    document_ids: List of str, int (Optional)\n",
    "        A unique value per document that will be used for referring to\n",
    "        documents in search results. If ids are not given to the model, the\n",
    "        index of each document in the original corpus will become the id.\n",
    "    keep_documents: bool (Optional, default True)\n",
    "        If set to False documents will only be used for training and not saved\n",
    "        as part of the model. This will reduce model size. When using search\n",
    "        functions only document ids will be returned, not the actual\n",
    "        documents.\n",
    "    workers: int (Optional)\n",
    "        The amount of worker threads to be used in training the model. Larger\n",
    "        amount will lead to faster training.\n",
    "    \n",
    "    tokenizer: callable (Optional, default None)\n",
    "        Override the default tokenization method. If None then\n",
    "        gensim.utils.simple_preprocess will be used.\n",
    "    use_embedding_model_tokenizer: bool (Optional, default False)\n",
    "        If using an embedding model other than doc2vec, use the model's\n",
    "        tokenizer for document embedding. If set to True the tokenizer, either\n",
    "        default or passed callable will be used to tokenize the text to\n",
    "        extract the vocabulary for word embedding.\n",
    "    umap_args: dict (Optional, default None)\n",
    "        Pass custom arguments to UMAP.\n",
    "    hdbscan_args: dict (Optional, default None)\n",
    "        Pass custom arguments to HDBSCAN.\n",
    "    \n",
    "    verbose: bool (Optional, default True)\n",
    "        Whether to print status data during training.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 documents,\n",
    "                 min_count=50,\n",
    "                 embedding_model='doc2vec',\n",
    "                 embedding_model_path=None,\n",
    "                 speed='learn',\n",
    "                 use_corpus_file=False,\n",
    "                 document_ids=None,\n",
    "                 keep_documents=True,\n",
    "                 workers=None,\n",
    "                 tokenizer=None,\n",
    "                 use_embedding_model_tokenizer=False,\n",
    "                 umap_args=None,\n",
    "                 hdbscan_args=None,\n",
    "                 verbose=True\n",
    "                 ):\n",
    "\n",
    "        if verbose:\n",
    "            logger.setLevel(logging.DEBUG)\n",
    "            self.verbose = True\n",
    "        else:\n",
    "            logger.setLevel(logging.WARNING)\n",
    "            self.verbose = False\n",
    "\n",
    "        if tokenizer is None:\n",
    "            tokenizer = default_tokenizer\n",
    "\n",
    "        # validate documents\n",
    "        if not (isinstance(documents, list) or isinstance(documents, np.ndarray)):\n",
    "            raise ValueError(\"Documents need to be a list of strings\")\n",
    "        if not all((isinstance(doc, str) or isinstance(doc, np.str_)) for doc in documents):\n",
    "            raise ValueError(\"Documents need to be a list of strings\")\n",
    "        if keep_documents:\n",
    "            self.documents = np.array(documents, dtype=\"object\")\n",
    "        else:\n",
    "            self.documents = None\n",
    "\n",
    "        # validate document ids\n",
    "        if document_ids is not None:\n",
    "            if not (isinstance(document_ids, list) or isinstance(document_ids, np.ndarray)):\n",
    "                raise ValueError(\"Documents ids need to be a list of str or int\")\n",
    "\n",
    "            if len(documents) != len(document_ids):\n",
    "                raise ValueError(\"Document ids need to match number of documents\")\n",
    "            elif len(document_ids) != len(set(document_ids)):\n",
    "                raise ValueError(\"Document ids need to be unique\")\n",
    "\n",
    "            if all((isinstance(doc_id, str) or isinstance(doc_id, np.str_)) for doc_id in document_ids):\n",
    "                self.doc_id_type = np.str_\n",
    "            elif all((isinstance(doc_id, int) or isinstance(doc_id, np.int_)) for doc_id in document_ids):\n",
    "                self.doc_id_type = np.int_\n",
    "            else:\n",
    "                raise ValueError(\"Document ids need to be str or int\")\n",
    "\n",
    "            self.document_ids_provided = True\n",
    "            self.document_ids = np.array(document_ids)\n",
    "            self.doc_id2index = dict(zip(document_ids, list(range(0, len(document_ids)))))\n",
    "        else:\n",
    "            self.document_ids_provided = False\n",
    "            self.document_ids = np.array(range(0, len(documents)))\n",
    "            self.doc_id2index = dict(zip(self.document_ids, list(range(0, len(self.document_ids)))))\n",
    "            self.doc_id_type = np.int_\n",
    "\n",
    "        acceptable_embedding_models = [\"universal-sentence-encoder-multilingual\",\n",
    "                                       \"universal-sentence-encoder\",\n",
    "                                       \"all-mpnet-base-v2\"]\n",
    "\n",
    "        self.embedding_model_path = embedding_model_path\n",
    "\n",
    "        if embedding_model == 'doc2vec':\n",
    "\n",
    "            # validate training inputs\n",
    "            if speed == \"fast-learn\":\n",
    "                hs = 0\n",
    "                negative = 5\n",
    "                epochs = 40\n",
    "            elif speed == \"learn\":\n",
    "                hs = 1\n",
    "                negative = 0\n",
    "                epochs = 40\n",
    "            elif speed == \"deep-learn\":\n",
    "                hs = 1\n",
    "                negative = 0\n",
    "                epochs = 400\n",
    "            elif speed == \"test-learn\":\n",
    "                hs = 0\n",
    "                negative = 5\n",
    "                epochs = 1\n",
    "            else:\n",
    "                raise ValueError(\"speed parameter needs to be one of: fast-learn, learn or deep-learn\")\n",
    "\n",
    "            if workers is None:\n",
    "                pass\n",
    "            elif isinstance(workers, int):\n",
    "                pass\n",
    "            else:\n",
    "                raise ValueError(\"workers needs to be an int\")\n",
    "\n",
    "            doc2vec_args = {\"vector_size\": 300,\n",
    "                            \"min_count\": min_count,\n",
    "                            \"window\": 15,\n",
    "                            \"sample\": 1e-5,\n",
    "                            \"negative\": negative,\n",
    "                            \"hs\": hs,\n",
    "                            \"epochs\": epochs,\n",
    "                            \"dm\": 0,\n",
    "                            \"dbow_words\": 1}\n",
    "\n",
    "            if workers is not None:\n",
    "                doc2vec_args[\"workers\"] = workers\n",
    "\n",
    "            logger.info('Pre-processing documents for training')\n",
    "\n",
    "            if use_corpus_file:\n",
    "                processed = [' '.join(tokenizer(doc)) for doc in documents]\n",
    "                lines = \"\\n\".join(processed)\n",
    "                temp = tempfile.NamedTemporaryFile(mode='w+t')\n",
    "                temp.write(lines)\n",
    "                doc2vec_args[\"corpus_file\"] = temp.name\n",
    "\n",
    "\n",
    "            else:\n",
    "                train_corpus = [TaggedDocument(tokenizer(doc), [i]) for i, doc in enumerate(documents)]\n",
    "                doc2vec_args[\"documents\"] = train_corpus\n",
    "\n",
    "            logger.info('Creating joint document/word embedding')\n",
    "            self.embedding_model = 'doc2vec'\n",
    "            self.model = Doc2Vec(**doc2vec_args)\n",
    "\n",
    "            if use_corpus_file:\n",
    "                temp.close()\n",
    "\n",
    "        elif embedding_model in acceptable_embedding_models:\n",
    "\n",
    "            self.embed = None\n",
    "            self.embedding_model = embedding_model\n",
    "\n",
    "            self._check_import_status()\n",
    "\n",
    "            logger.info('Pre-processing documents for training')\n",
    "\n",
    "            # preprocess documents\n",
    "            tokenized_corpus = [tokenizer(doc) for doc in documents]\n",
    "\n",
    "            def return_doc(doc):\n",
    "                return doc\n",
    "\n",
    "            # preprocess vocabulary\n",
    "            vectorizer = CountVectorizer(tokenizer=return_doc, preprocessor=return_doc)\n",
    "            doc_word_counts = vectorizer.fit_transform(tokenized_corpus)\n",
    "            words = vectorizer.get_feature_names()\n",
    "            word_counts = np.array(np.sum(doc_word_counts, axis=0).tolist()[0])\n",
    "            vocab_inds = np.where(word_counts > min_count)[0]\n",
    "\n",
    "            if len(vocab_inds) == 0:\n",
    "                raise ValueError(f\"A min_count of {min_count} results in \"\n",
    "                                 f\"all words being ignored, choose a lower value.\")\n",
    "            self.vocab = [words[ind] for ind in vocab_inds]\n",
    "\n",
    "            self._check_model_status()\n",
    "\n",
    "            logger.info('Creating joint document/word embedding')\n",
    "\n",
    "            # embed words\n",
    "            self.word_indexes = dict(zip(self.vocab, range(len(self.vocab))))\n",
    "            self.word_vectors = self._l2_normalize(np.array(self.embed(self.vocab)))\n",
    "\n",
    "            # embed documents\n",
    "            if use_embedding_model_tokenizer:\n",
    "                self.document_vectors = self._embed_documents(documents)\n",
    "            else:\n",
    "                train_corpus = [' '.join(tokens) for tokens in tokenized_corpus]\n",
    "                self.document_vectors = self._embed_documents(train_corpus)\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"{embedding_model} is an invalid embedding model.\")\n",
    "\n",
    "        # create 5D embeddings of documents\n",
    "        logger.info('Creating lower dimension embedding of documents')\n",
    "\n",
    "        if umap_args is None:\n",
    "            umap_args = {'n_neighbors': 15,\n",
    "                         'n_components': 5,\n",
    "                         'metric': 'cosine'}\n",
    "\n",
    "        umap_model = umap.UMAP(**umap_args).fit(self._get_document_vectors(norm=False))\n",
    "\n",
    "        # find dense areas of document vectors\n",
    "        logger.info('Finding dense areas of documents')\n",
    "\n",
    "        if hdbscan_args is None:\n",
    "            hdbscan_args = {'min_cluster_size': 15,\n",
    "                            'metric': 'euclidean',\n",
    "                            'cluster_selection_method': 'eom'}\n",
    "\n",
    "        cluster = hdbscan.HDBSCAN(**hdbscan_args).fit(umap_model.embedding_)\n",
    "\n",
    "        # calculate topic vectors from dense areas of documents\n",
    "        logger.info('Finding topics')\n",
    "\n",
    "        # create topic vectors\n",
    "        self._create_topic_vectors(cluster.labels_)\n",
    "\n",
    "        # deduplicate topics\n",
    "        self._deduplicate_topics()\n",
    "\n",
    "        # find topic words and scores\n",
    "        self.topic_words, self.topic_word_scores = self._find_topic_words_and_scores(topic_vectors=self.topic_vectors)\n",
    "\n",
    "        # assign documents to topic\n",
    "        self.doc_top, self.doc_dist = self._calculate_documents_topic(self.topic_vectors,\n",
    "                                                                      self._get_document_vectors())\n",
    "\n",
    "        # calculate topic sizes\n",
    "        self.topic_sizes = self._calculate_topic_sizes(hierarchy=False)\n",
    "\n",
    "        # re-order topics\n",
    "        self._reorder_topics(hierarchy=False)\n",
    "\n",
    "        # initialize variables for hierarchical topic reduction\n",
    "        self.topic_vectors_reduced = None\n",
    "        self.doc_top_reduced = None\n",
    "        self.doc_dist_reduced = None\n",
    "        self.topic_sizes_reduced = None\n",
    "        self.topic_words_reduced = None\n",
    "        self.topic_word_scores_reduced = None\n",
    "        self.hierarchy = None\n",
    "\n",
    "        # initialize document indexing variables\n",
    "        self.document_index = None\n",
    "        self.serialized_document_index = None\n",
    "        self.documents_indexed = False\n",
    "        self.index_id2doc_id = None\n",
    "        self.doc_id2index_id = None\n",
    "\n",
    "        # initialize word indexing variables\n",
    "        self.word_index = None\n",
    "        self.serialized_word_index = None\n",
    "        self.words_indexed = False\n",
    "\n",
    "    def _check_import_status(self):\n",
    "        if self.embedding_model != 'all-mpnet-base-v2':\n",
    "            if not _HAVE_TENSORFLOW:\n",
    "                raise ImportError(f\"{self.embedding_model} is not available.\\n\\n\"\n",
    "                                  \"Try: pip install top2vec[sentence_encoders]\\n\\n\"\n",
    "                                  \"Alternatively try: pip install tensorflow tensorflow_hub tensorflow_text\")\n",
    "        else:\n",
    "            if not _HAVE_TORCH:\n",
    "                raise ImportError(f\"{self.embedding_model} is not available.\\n\\n\"\n",
    "                                  \"Try: pip install top2vec[sentence_transformers]\\n\\n\"\n",
    "                                  \"Alternatively try: pip install torch sentence_transformers\")\n",
    "\n",
    "    def _check_model_status(self):\n",
    "        if self.embed is None:\n",
    "            if self.verbose is False:\n",
    "                logger.setLevel(logging.DEBUG)\n",
    "\n",
    "            if self.embedding_model != \"all-mpnet-base-v2\":\n",
    "                if self.embedding_model_path is None:\n",
    "                    logger.info(f'Downloading {self.embedding_model} model')\n",
    "                    if self.embedding_model == \"universal-sentence-encoder-multilingual\":\n",
    "                        module = \"https://tfhub.dev/google/universal-sentence-encoder-multilingual/3\"\n",
    "                    else:\n",
    "                        module = \"https://tfhub.dev/google/universal-sentence-encoder/4\"\n",
    "                else:\n",
    "                    logger.info(f'Loading {self.embedding_model} model at {self.embedding_model_path}')\n",
    "                    module = self.embedding_model_path\n",
    "                self.embed = hub.load(module)\n",
    "\n",
    "            else:\n",
    "                if self.embedding_model_path is None:\n",
    "                    logger.info(f'Downloading {self.embedding_model} model')\n",
    "                    module = 'all-mpnet-base-v2'\n",
    "                else:\n",
    "                    logger.info(f'Loading {self.embedding_model} model at {self.embedding_model_path}')\n",
    "                    module = self.embedding_model_path\n",
    "                model = SentenceTransformer(module)\n",
    "                self.embed = model.encode\n",
    "\n",
    "        if self.verbose is False:\n",
    "            logger.setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oz16gmSjHcs6"
   },
   "source": [
    "We can then use this `Top2VecNew` class to run our experiments including the `\"all-mpnet-base-v2\"` model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, custom = \"20NewsGroup\", False\n",
    "params = {\"nr_topics\": [15, 20, 25, 30],\n",
    "            # \"embedding_model\": \"all-mpnet-base-v2\",\n",
    "            \"embedding_model\": \"doc2vec\",\n",
    "            \"hdbscan_args\": {'min_cluster_size': 15,\n",
    "                            'metric': 'euclidean',\n",
    "                            'cluster_selection_method': 'eom'}}\n",
    "\n",
    "trainer = Trainer(dataset=dataset,\n",
    "                    custom_dataset=custom,\n",
    "                    custom_model=Top2VecNew,\n",
    "                    model_name=\"Top2Vec\",\n",
    "                    params=params,\n",
    "                    verbose=True)\n",
    "results = trainer.train(save=f\"Top2Vec_20news_mpnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "NdPp3Xsda_SJ"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-22 02:44:29,410 - top2vec - INFO - Pre-processing documents for training\n",
      "2022-04-22 02:44:29,410 - top2vec - INFO - Pre-processing documents for training\n",
      "2022-04-22 02:44:29,410 - top2vec - INFO - Pre-processing documents for training\n",
      "2022-04-22 02:44:30,871 - top2vec - INFO - Downloading all-mpnet-base-v2 model\n",
      "2022-04-22 02:44:30,871 - top2vec - INFO - Downloading all-mpnet-base-v2 model\n",
      "2022-04-22 02:44:30,871 - top2vec - INFO - Downloading all-mpnet-base-v2 model\n",
      "2022-04-22 02:44:37,994 - top2vec - INFO - Creating joint document/word embedding\n",
      "2022-04-22 02:44:37,994 - top2vec - INFO - Creating joint document/word embedding\n",
      "2022-04-22 02:44:37,994 - top2vec - INFO - Creating joint document/word embedding\n",
      "2022-04-22 02:45:09,237 - top2vec - INFO - Creating lower dimension embedding of documents\n",
      "2022-04-22 02:45:09,237 - top2vec - INFO - Creating lower dimension embedding of documents\n",
      "2022-04-22 02:45:09,237 - top2vec - INFO - Creating lower dimension embedding of documents\n",
      "2022-04-22 02:45:15,515 - top2vec - INFO - Finding dense areas of documents\n",
      "2022-04-22 02:45:15,515 - top2vec - INFO - Finding dense areas of documents\n",
      "2022-04-22 02:45:15,515 - top2vec - INFO - Finding dense areas of documents\n",
      "2022-04-22 02:45:15,868 - top2vec - INFO - Finding topics\n",
      "2022-04-22 02:45:15,868 - top2vec - INFO - Finding topics\n",
      "2022-04-22 02:45:15,868 - top2vec - INFO - Finding topics\n",
      "2022-04-22 02:45:20,157 - top2vec - INFO - Pre-processing documents for training\n",
      "2022-04-22 02:45:20,157 - top2vec - INFO - Pre-processing documents for training\n",
      "2022-04-22 02:45:20,157 - top2vec - INFO - Pre-processing documents for training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "========================================================================================================================\n",
      "npmi: 0.03759252973250017\n",
      "diversity: 1.0\n",
      "cv: 0.5649855350007895\n",
      "uci: -0.8163111925051703\n",
      " \n",
      "Topic 1 cotains words: ['implement', 'implementation', 'inform', 'information', 'organization', 'technology', 'document', 'communication', 'management', 'design']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 2 cotains words: ['baseball', 'hockey', 'pitch', 'league', 'sport', 'win', 'game', 'playoff', 'lose', 'team']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-22 02:45:21,545 - top2vec - INFO - Downloading all-mpnet-base-v2 model\n",
      "2022-04-22 02:45:21,545 - top2vec - INFO - Downloading all-mpnet-base-v2 model\n",
      "2022-04-22 02:45:21,545 - top2vec - INFO - Downloading all-mpnet-base-v2 model\n",
      "2022-04-22 02:45:27,945 - top2vec - INFO - Creating joint document/word embedding\n",
      "2022-04-22 02:45:27,945 - top2vec - INFO - Creating joint document/word embedding\n",
      "2022-04-22 02:45:27,945 - top2vec - INFO - Creating joint document/word embedding\n",
      "2022-04-22 02:45:52,991 - top2vec - INFO - Creating lower dimension embedding of documents\n",
      "2022-04-22 02:45:52,991 - top2vec - INFO - Creating lower dimension embedding of documents\n",
      "2022-04-22 02:45:52,991 - top2vec - INFO - Creating lower dimension embedding of documents\n",
      "2022-04-22 02:45:58,940 - top2vec - INFO - Finding dense areas of documents\n",
      "2022-04-22 02:45:58,940 - top2vec - INFO - Finding dense areas of documents\n",
      "2022-04-22 02:45:58,940 - top2vec - INFO - Finding dense areas of documents\n",
      "2022-04-22 02:45:59,279 - top2vec - INFO - Finding topics\n",
      "2022-04-22 02:45:59,279 - top2vec - INFO - Finding topics\n",
      "2022-04-22 02:45:59,279 - top2vec - INFO - Finding topics\n",
      "2022-04-22 02:46:12,023 - top2vec - INFO - Pre-processing documents for training\n",
      "2022-04-22 02:46:12,023 - top2vec - INFO - Pre-processing documents for training\n",
      "2022-04-22 02:46:12,023 - top2vec - INFO - Pre-processing documents for training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "========================================================================================================================\n",
      "npmi: 0.07704017576768464\n",
      "diversity: 0.805\n",
      "cv: 0.6259903075076161\n",
      "uci: -0.22003464845688986\n",
      " \n",
      "Topic 1 cotains words: ['baseball', 'hockey', 'pitch', 'league', 'sport', 'win', 'game', 'lose', 'playoff', 'team']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 2 cotains words: ['vehicle', 'car', 'engine', 'motorcycle', 'truck', 'auto', 'bike', 'motor', 'ride', 'drive']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 3 cotains words: ['religion', 'christianity', 'religious', 'scripture', 'philosophy', 'doctrine', 'belief', 'church', 'christian', 'biblical']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 4 cotains words: ['government', 'society', 'organization', 'political', 'implement', 'topic', 'administration', 'inform', 'introduction', 'document']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 5 cotains words: ['software', 'program', 'programming', 'unix', 'interface', 'implement', 'format', 'compile', 'application', 'implementation']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 6 cotains words: ['gun', 'police', 'crime', 'arrest', 'firearm', 'law', 'enforcement', 'officer', 'investigation', 'court']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 7 cotains words: ['business', 'organization', 'management', 'implement', 'implementation', 'company', 'industry', 'administration', 'development', 'organize']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 8 cotains words: ['modem', 'hardware', 'motherboard', 'processor', 'computer', 'device', 'cpu', 'ram', 'configuration', 'technology']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 9 cotains words: ['email', 'mail', 'mailing', 'newsgroup', 'letter', 'inform', 'message', 'announcement', 'communication', 'contact']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 10 cotains words: ['issue', 'problem', 'bug', 'fix', 'instal', 'compile', 'program', 'repair', 'error', 'configuration']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 11 cotains words: ['sale', 'sell', 'price', 'purchase', 'buy', 'vendor', 'market', 'trade', 'shipping', 'exchange']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 12 cotains words: ['genocide', 'terrorist', 'war', 'conflict', 'arab', 'israeli', 'language', 'government', 'political', 'history']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 13 cotains words: ['medical', 'medicine', 'doctor', 'health', 'treatment', 'patient', 'procedure', 'hospital', 'aid', 'disease']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 14 cotains words: ['vga', 'monitor', 'display', 'screen', 'resolution', 'hardware', 'computer', 'interface', 'workstation', 'cpu']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 15 cotains words: ['evidence', 'philosophy', 'observation', 'science', 'scientific', 'conclusion', 'research', 'knowledge', 'introduction', 'investigation']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 16 cotains words: ['orbit', 'space', 'shuttle', 'engineering', 'implement', 'launch', 'implementation', 'satellite', 'project', 'moon']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 17 cotains words: ['audio', 'sound', 'surround', 'circuit', 'implement', 'signal', 'technology', 'hardware', 'speaker', 'tech']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 18 cotains words: ['disk', 'hardware', 'floppy', 'ide', 'ram', 'workstation', 'scsi', 'computer', 'motherboard', 'installation']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 19 cotains words: ['encryption', 'security', 'encrypt', 'implementation', 'implement', 'privacy', 'secure', 'technology', 'information', 'algorithm']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 20 cotains words: ['printer', 'print', 'hardware', 'computer', 'workstation', 'device', 'technology', 'software', 'tech', 'format']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-22 02:46:13,475 - top2vec - INFO - Downloading all-mpnet-base-v2 model\n",
      "2022-04-22 02:46:13,475 - top2vec - INFO - Downloading all-mpnet-base-v2 model\n",
      "2022-04-22 02:46:13,475 - top2vec - INFO - Downloading all-mpnet-base-v2 model\n",
      "2022-04-22 02:46:19,841 - top2vec - INFO - Creating joint document/word embedding\n",
      "2022-04-22 02:46:19,841 - top2vec - INFO - Creating joint document/word embedding\n",
      "2022-04-22 02:46:19,841 - top2vec - INFO - Creating joint document/word embedding\n",
      "2022-04-22 02:46:44,849 - top2vec - INFO - Creating lower dimension embedding of documents\n",
      "2022-04-22 02:46:44,849 - top2vec - INFO - Creating lower dimension embedding of documents\n",
      "2022-04-22 02:46:44,849 - top2vec - INFO - Creating lower dimension embedding of documents\n",
      "2022-04-22 02:46:50,752 - top2vec - INFO - Finding dense areas of documents\n",
      "2022-04-22 02:46:50,752 - top2vec - INFO - Finding dense areas of documents\n",
      "2022-04-22 02:46:50,752 - top2vec - INFO - Finding dense areas of documents\n",
      "2022-04-22 02:46:51,086 - top2vec - INFO - Finding topics\n",
      "2022-04-22 02:46:51,086 - top2vec - INFO - Finding topics\n",
      "2022-04-22 02:46:51,086 - top2vec - INFO - Finding topics\n",
      "2022-04-22 02:47:05,651 - top2vec - INFO - Pre-processing documents for training\n",
      "2022-04-22 02:47:05,651 - top2vec - INFO - Pre-processing documents for training\n",
      "2022-04-22 02:47:05,651 - top2vec - INFO - Pre-processing documents for training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "========================================================================================================================\n",
      "npmi: 0.08293612989183873\n",
      "diversity: 0.772\n",
      "cv: 0.6175307408249386\n",
      "uci: -0.005300180484552393\n",
      " \n",
      "Topic 1 cotains words: ['baseball', 'hockey', 'pitch', 'league', 'sport', 'game', 'win', 'playoff', 'lose', 'team']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 2 cotains words: ['vehicle', 'car', 'engine', 'motorcycle', 'truck', 'auto', 'motor', 'bike', 'drive', 'ride']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 3 cotains words: ['religion', 'christianity', 'religious', 'scripture', 'philosophy', 'doctrine', 'belief', 'church', 'christian', 'homosexuality']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 4 cotains words: ['documentation', 'software', 'shareware', 'implement', 'technology', 'implementation', 'format', 'interface', 'programming', 'document']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 5 cotains words: ['newsgroup', 'inform', 'information', 'text', 'topic', 'newspaper', 'communication', 'letter', 'announcement', 'read']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 6 cotains words: ['society', 'religion', 'language', 'philosophy', 'culture', 'topic', 'organization', 'literature', 'government', 'inform']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 7 cotains words: ['arrest', 'investigation', 'police', 'crime', 'law', 'court', 'complaint', 'enforcement', 'officer', 'assault']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 8 cotains words: ['window', 'program', 'compile', 'ide', 'software', 'application', 'unix', 'configuration', 'terminal', 'programming']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 9 cotains words: ['hardware', 'motherboard', 'processor', 'cpu', 'computer', 'ram', 'workstation', 'device', 'printer', 'modem']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 10 cotains words: ['sale', 'sell', 'price', 'purchase', 'buy', 'market', 'trade', 'vendor', 'exchange', 'offer']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 11 cotains words: ['genocide', 'terrorist', 'arab', 'war', 'israeli', 'conflict', 'language', 'government', 'political', 'history']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 12 cotains words: ['issue', 'problem', 'repair', 'computer', 'crash', 'bug', 'fix', 'ram', 'instal', 'solution']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 13 cotains words: ['medical', 'medicine', 'doctor', 'health', 'treatment', 'patient', 'procedure', 'hospital', 'disease', 'aid']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 14 cotains words: ['format', 'software', 'binary', 'algorithm', 'program', 'file', 'display', 'programming', 'interface', 'render']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 15 cotains words: ['management', 'business', 'organization', 'implement', 'implementation', 'job', 'company', 'work', 'administration', 'industry']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 16 cotains words: ['vga', 'monitor', 'display', 'screen', 'resolution', 'hardware', 'computer', 'interface', 'workstation', 'configuration']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 17 cotains words: ['orbit', 'space', 'shuttle', 'engineering', 'launch', 'implement', 'satellite', 'moon', 'project', 'implementation']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 18 cotains words: ['disk', 'hardware', 'floppy', 'ide', 'scsi', 'workstation', 'ram', 'installation', 'computer', 'storage']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 19 cotains words: ['government', 'administration', 'political', 'organization', 'economic', 'implement', 'politic', 'policy', 'establish', 'constitution']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 20 cotains words: ['philosophy', 'science', 'scientific', 'religion', 'introduction', 'evidence', 'knowledge', 'research', 'literature', 'belief']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 21 cotains words: ['audio', 'sound', 'surround', 'hardware', 'speaker', 'technology', 'device', 'radio', 'tech', 'circuit']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 22 cotains words: ['encryption', 'security', 'encrypt', 'implementation', 'implement', 'privacy', 'secure', 'technology', 'information', 'algorithm']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 23 cotains words: ['mail', 'email', 'mailing', 'message', 'send', 'contact', 'address', 'receive', 'letter', 'reply']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 24 cotains words: ['modem', 'telephone', 'fax', 'phone', 'device', 'port', 'call', 'interface', 'configuration', 'signal']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 25 cotains words: ['gun', 'firearm', 'armed', 'weapon', 'police', 'bullet', 'crime', 'government', 'security', 'safety']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-22 02:47:07,143 - top2vec - INFO - Downloading all-mpnet-base-v2 model\n",
      "2022-04-22 02:47:07,143 - top2vec - INFO - Downloading all-mpnet-base-v2 model\n",
      "2022-04-22 02:47:07,143 - top2vec - INFO - Downloading all-mpnet-base-v2 model\n",
      "2022-04-22 02:47:13,523 - top2vec - INFO - Creating joint document/word embedding\n",
      "2022-04-22 02:47:13,523 - top2vec - INFO - Creating joint document/word embedding\n",
      "2022-04-22 02:47:13,523 - top2vec - INFO - Creating joint document/word embedding\n",
      "2022-04-22 02:47:41,711 - top2vec - INFO - Creating lower dimension embedding of documents\n",
      "2022-04-22 02:47:41,711 - top2vec - INFO - Creating lower dimension embedding of documents\n",
      "2022-04-22 02:47:41,711 - top2vec - INFO - Creating lower dimension embedding of documents\n",
      "2022-04-22 02:47:47,540 - top2vec - INFO - Finding dense areas of documents\n",
      "2022-04-22 02:47:47,540 - top2vec - INFO - Finding dense areas of documents\n",
      "2022-04-22 02:47:47,540 - top2vec - INFO - Finding dense areas of documents\n",
      "2022-04-22 02:47:47,895 - top2vec - INFO - Finding topics\n",
      "2022-04-22 02:47:47,895 - top2vec - INFO - Finding topics\n",
      "2022-04-22 02:47:47,895 - top2vec - INFO - Finding topics\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results\n",
      "========================================================================================================================\n",
      "npmi: 0.07071040552770502\n",
      "diversity: 0.6833333333333333\n",
      "cv: 0.61614431471854\n",
      "uci: -0.2832266366240988\n",
      " \n",
      "Topic 1 cotains words: ['baseball', 'hockey', 'pitch', 'league', 'sport', 'win', 'game', 'playoff', 'lose', 'team']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 2 cotains words: ['religion', 'christianity', 'religious', 'scripture', 'philosophy', 'doctrine', 'belief', 'church', 'christian', 'biblical']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 3 cotains words: ['vehicle', 'car', 'engine', 'auto', 'truck', 'motor', 'driver', 'drive', 'manual', 'repair']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 4 cotains words: ['inform', 'topic', 'text', 'information', 'newspaper', 'newsgroup', 'write', 'communication', 'letter', 'writing']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 5 cotains words: ['society', 'government', 'political', 'religion', 'organization', 'philosophy', 'culture', 'topic', 'language', 'implement']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 6 cotains words: ['arrest', 'investigation', 'police', 'crime', 'court', 'law', 'complaint', 'enforcement', 'trial', 'warrant']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 7 cotains words: ['genocide', 'terrorist', 'war', 'arab', 'conflict', 'israeli', 'language', 'government', 'political', 'history']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 8 cotains words: ['organization', 'business', 'management', 'implement', 'implementation', 'organize', 'company', 'documentation', 'administration', 'industry']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 9 cotains words: ['sale', 'sell', 'buy', 'purchase', 'price', 'market', 'trade', 'vendor', 'offer', 'exchange']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 10 cotains words: ['medical', 'medicine', 'doctor', 'health', 'treatment', 'patient', 'procedure', 'hospital', 'disease', 'aid']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 11 cotains words: ['hardware', 'computer', 'device', 'workstation', 'tech', 'software', 'printer', 'technology', 'interface', 'utility']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 12 cotains words: ['encryption', 'security', 'encrypt', 'implementation', 'implement', 'privacy', 'secure', 'technology', 'information', 'shareware']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 13 cotains words: ['orbit', 'shuttle', 'space', 'launch', 'engineering', 'implement', 'satellite', 'moon', 'project', 'implementation']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 14 cotains words: ['vga', 'monitor', 'display', 'screen', 'resolution', 'hardware', 'computer', 'interface', 'workstation', 'cpu']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 15 cotains words: ['philosophy', 'science', 'scientific', 'religion', 'introduction', 'knowledge', 'evidence', 'belief', 'observation', 'research']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 16 cotains words: ['sound', 'audio', 'surround', 'hardware', 'circuit', 'technology', 'implement', 'tech', 'speaker', 'component']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 17 cotains words: ['disk', 'hardware', 'floppy', 'ide', 'workstation', 'scsi', 'ram', 'installation', 'computer', 'motherboard']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 18 cotains words: ['motherboard', 'hardware', 'processor', 'cpu', 'ram', 'computer', 'modem', 'device', 'mhz', 'configuration']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 19 cotains words: ['bike', 'motorcycle', 'ride', 'vehicle', 'cycle', 'tire', 'gear', 'car', 'truck', 'rear']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 20 cotains words: ['mail', 'email', 'mailing', 'message', 'contact', 'send', 'address', 'letter', 'receive', 'reply']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 21 cotains words: ['format', 'file', 'document', 'print', 'software', 'printer', 'unix', 'compile', 'documentation', 'copy']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 22 cotains words: ['government', 'administration', 'organization', 'inform', 'policy', 'implement', 'economic', 'political', 'department', 'establish']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 23 cotains words: ['display', 'algorithm', 'design', 'draw', 'screen', 'interface', 'implement', 'implementation', 'render', 'pattern']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 24 cotains words: ['issue', 'computer', 'crash', 'problem', 'repair', 'ram', 'bug', 'ide', 'program', 'instal']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 25 cotains words: ['gun', 'firearm', 'weapon', 'armed', 'police', 'crime', 'bullet', 'government', 'security', 'safety']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 26 cotains words: ['modem', 'telephone', 'fax', 'phone', 'device', 'port', 'interface', 'call', 'cable', 'configuration']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 27 cotains words: ['software', 'programming', 'implementation', 'program', 'technology', 'documentation', 'implement', 'development', 'interface', 'shareware']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 28 cotains words: ['compile', 'issue', 'problem', 'error', 'fix', 'instal', 'bug', 'program', 'build', 'configuration']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 29 cotains words: ['unix', 'workstation', 'shareware', 'software', 'program', 'interface', 'programming', 'configuration', 'implementation', 'implement']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Topic 30 cotains words: ['window', 'program', 'application', 'ide', 'interface', 'terminal', 'widget', 'display', 'screen', 'software']\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      " \n"
     ]
    }
   ],
   "source": [
    "dataset, custom = \"20NewsGroup\", False\n",
    "params = {\"nr_topics\": [15, 20, 25, 30],\n",
    "            \"embedding_model\": \"all-mpnet-base-v2\",\n",
    "            # \"embedding_model\": \"doc2vec\",\n",
    "            \"hdbscan_args\": {'min_cluster_size': 15,\n",
    "                            'metric': 'euclidean',\n",
    "                            'cluster_selection_method': 'eom'}}\n",
    "\n",
    "trainer = Trainer(dataset=dataset,\n",
    "                    custom_dataset=custom,\n",
    "                    custom_model=Top2VecNew,\n",
    "                    model_name=\"Top2Vec\",\n",
    "                    params=params,\n",
    "                    verbose=True)\n",
    "results = trainer.train(save=f\"Top2Vec_20news_mpnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "BERTopic OCTIS v3.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "interpreter": {
   "hash": "a43e9924ed8c5ecb71f87ad8ea3cd8cfd1f8f187d2b016fe082beb639477b0f5"
  },
  "kernelspec": {
   "display_name": "bertopic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
